{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7e1b44",
   "metadata": {},
   "source": [
    "#### This notebook implements the Value Iteration Algo and applies it to each provided MDP file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbc2ae",
   "metadata": {},
   "source": [
    "##### Including numpy and the defaultdict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1aaa235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict # (automatically initializes missing keys with a default value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f6c37",
   "metadata": {},
   "source": [
    "##### Defining the MDP class and the value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51d9d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP: #this class loads an MDP from a txt file and applies the value iteration algo\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.parse_mdp()\n",
    "\n",
    "    def parse_mdp(self): #prsing the file\n",
    "        with open(self.filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        self.num_states = int(lines[0].split()[1]) #need to convert to integer for it to work\n",
    "        self.num_actions = int(lines[1].split()[1])\n",
    "\n",
    "        end_line = lines[2].split()\n",
    "\n",
    "        if end_line[1] == '-1':\n",
    "            self.end_states = []\n",
    "        else:\n",
    "            self.end_states = [int(x) for x in end_line[1:]]\n",
    "\n",
    "        #making 3 nested defaultdict to map state action pairs to next state, rewards and transition probabilities\n",
    "        self.transitions = defaultdict(lambda: defaultdict(list))\n",
    "        self.rewards = defaultdict(lambda: defaultdict(list))\n",
    "        self.probabilities = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        #startwith function clutch\n",
    "        for line in lines[3:]:\n",
    "            if line.startswith('transition'):\n",
    "                parts = line.split()\n",
    "                s, a, s_, r, p = int(parts[1]), int(parts[2]), int(parts[3]), float(parts[4]), float(parts[5])\n",
    "                self.transitions[s][a].append(s_)\n",
    "                self.rewards[s][a].append(r)\n",
    "                self.probabilities[s][a].append(p)\n",
    "            elif line.startswith('mdptype'):\n",
    "                self.mdp_type = line.split()[1]\n",
    "            elif line.startswith('discount'):\n",
    "                self.discount = float(line.split()[1])\n",
    "\n",
    "    def get_expected_reward(self, s, a): #function to get immediate expected reward after taking a variable action, also the zip function makes our lives easier\n",
    "        if a not in self.rewards[s]: return 0.0\n",
    "        return sum(r * p for r, p in zip(self.rewards[s][a], self.probabilities[s][a]))\n",
    "    \n",
    "    def get_expected_value(self, s, a, V): #updating all the value functions by a onestep search\n",
    "        if a not in self.transitions[s]: return 0.0\n",
    "        return sum(p * V[s_] for s_, p in zip(self.transitions[s][a], self.probabilities[s][a]))\n",
    "    \n",
    "    def value_iteration(self, max_iterations=10000, tol=1e-8):\n",
    "        V = np.zeros(self.num_states)\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            V_new = np.zeros_like(V)\n",
    "            delta = 0.0\n",
    "\n",
    "            for s in range(self.num_states):\n",
    "                if s in self.end_states:\n",
    "                    V_new[s] = 0.0\n",
    "                else:\n",
    "                    Qsa = [ #here 'Qsa' is the action value function\n",
    "                        self.get_expected_reward(s, a) + self.discount * self.get_expected_value(s, a, V)\n",
    "                        for a in range(self.num_actions)\n",
    "                    ]\n",
    "                    V_new[s] = max(Qsa) if Qsa else 0.0 #the value of state value function is the value of the maximum action value function\n",
    "                delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "            V = V_new\n",
    "\n",
    "            if delta < tol: #if the change is lesser than a really small number (tol here) then we have the best policy, or atleast we're close to the best policy\n",
    "                break\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def extract_policy(self, V): #getting the policy is finding which action has the highest action value function value so this function is the implementation of that\n",
    "        pi = np.zeros(self.num_states, dtype=int)\n",
    "        for s in range(self.num_states):\n",
    "            if s in self.end_states:\n",
    "                pi[s] = 0\n",
    "            else:\n",
    "                Qsa = [\n",
    "                    self.get_expected_reward(s, a) + self.discount * self.get_expected_value(s, a, V)\n",
    "                    for a in range(self.num_actions)\n",
    "                ]\n",
    "                pi[s] = np.argmax(Qsa) if Qsa else 0\n",
    "        return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37912ba0",
   "metadata": {},
   "source": [
    "###### Solution for data/continuing-mdp-2-2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d17eae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.999299 0\n",
      "5.918450 0\n"
     ]
    }
   ],
   "source": [
    "mdp1 = MDP('data/continuing-mdp-2-2.txt')\n",
    "V1 = mdp1.value_iteration()\n",
    "pi1 = mdp1.extract_policy(V1)\n",
    "for s in range(mdp1.num_states):\n",
    "    print(f\"{V1[s]:.6f} {pi1[s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037f69b",
   "metadata": {},
   "source": [
    "###### Solution for data/continuing-mdp-10-5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc27366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.234958 3\n",
      "2.373612 3\n",
      "2.604046 3\n",
      "2.647784 1\n",
      "2.522231 4\n",
      "2.375252 0\n",
      "2.684806 2\n",
      "2.688310 0\n",
      "2.640809 3\n",
      "2.572427 1\n"
     ]
    }
   ],
   "source": [
    "mdp2 = MDP('data/continuing-mdp-10-5.txt')\n",
    "V2 = mdp2.value_iteration()\n",
    "pi2 = mdp2.extract_policy(V2)\n",
    "for s in range(mdp2.num_states):\n",
    "    print(f\"{V2[s]:.6f} {pi2[s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e9906",
   "metadata": {},
   "source": [
    "###### Solution for data/continuing-mdp-50-20.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c91917b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.065079 7\n",
      "1.051696 2\n",
      "0.824259 7\n",
      "0.601320 14\n",
      "1.057797 4\n",
      "0.980877 19\n",
      "0.983041 18\n",
      "1.002595 5\n",
      "0.886921 15\n",
      "0.837798 8\n",
      "1.109280 8\n",
      "0.910305 19\n",
      "1.155357 7\n",
      "0.958098 8\n",
      "0.772395 18\n",
      "1.218694 16\n",
      "0.939597 11\n",
      "0.840961 19\n",
      "0.934034 2\n",
      "0.899851 12\n",
      "1.168103 14\n",
      "0.985183 19\n",
      "1.032489 14\n",
      "1.110618 15\n",
      "0.779151 0\n",
      "0.945382 1\n",
      "1.185461 3\n",
      "1.083733 18\n",
      "0.697620 15\n",
      "1.125198 5\n",
      "0.556266 1\n",
      "1.088646 6\n",
      "0.829482 11\n",
      "0.884322 6\n",
      "1.180251 1\n",
      "0.922217 4\n",
      "0.916141 11\n",
      "1.031048 10\n",
      "1.077761 14\n",
      "0.900197 19\n",
      "0.855533 5\n",
      "1.205419 0\n",
      "1.056961 4\n",
      "0.720773 14\n",
      "1.141582 1\n",
      "1.110485 4\n",
      "0.983264 5\n",
      "1.030596 3\n",
      "0.779689 1\n",
      "0.815195 12\n"
     ]
    }
   ],
   "source": [
    "mdp3 = MDP('data/continuing-mdp-50-20.txt')\n",
    "V3 = mdp3.value_iteration()\n",
    "pi3 = mdp3.extract_policy(V3)\n",
    "for s in range(mdp3.num_states):\n",
    "    print(f\"{V3[s]:.6f} {pi3[s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f58e4a",
   "metadata": {},
   "source": [
    "###### Solution for data/episodic-mdp-2-2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee235f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 0\n",
      "1.455816 0\n"
     ]
    }
   ],
   "source": [
    "mdp4 = MDP('data/episodic-mdp-2-2.txt')\n",
    "V4 = mdp4.value_iteration()\n",
    "pi4 = mdp4.extract_policy(V4)\n",
    "for s in range(mdp4.num_states):\n",
    "    print(f\"{V4[s]:.6f} {pi4[s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4dec1a",
   "metadata": {},
   "source": [
    "###### Solution for data/episodic-mdp-10-5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8a5401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 0\n",
      "525.256443 3\n",
      "525.548050 4\n",
      "500.064054 2\n",
      "468.517954 1\n",
      "0.000000 0\n",
      "522.022358 2\n",
      "513.605831 2\n",
      "351.127056 4\n",
      "524.331008 0\n"
     ]
    }
   ],
   "source": [
    "mdp5 = MDP('data/episodic-mdp-10-5.txt')\n",
    "V5 = mdp5.value_iteration()\n",
    "pi5 = mdp5.extract_policy(V5)\n",
    "for s in range(mdp5.num_states):\n",
    "    print(f\"{V5[s]:.6f} {pi5[s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21141768",
   "metadata": {},
   "source": [
    "###### Solution for data/episodic-mdp-50-20.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03323b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.985542 16\n",
      "7.837297 9\n",
      "0.000000 0\n",
      "7.664216 18\n",
      "7.830741 15\n",
      "7.826878 12\n",
      "7.943427 10\n",
      "8.261769 4\n",
      "7.869692 14\n",
      "8.348371 5\n",
      "7.711355 11\n",
      "7.775431 0\n",
      "7.914741 17\n",
      "8.006133 16\n",
      "8.101707 0\n",
      "8.089338 15\n",
      "0.000000 0\n",
      "7.652557 9\n",
      "8.124858 4\n",
      "7.843161 15\n",
      "8.415760 12\n",
      "7.321340 9\n",
      "7.627955 2\n",
      "7.984528 7\n",
      "7.708910 13\n",
      "7.777016 10\n",
      "8.089617 15\n",
      "5.340502 18\n",
      "8.238763 19\n",
      "7.855451 6\n",
      "7.457378 3\n",
      "7.829692 0\n",
      "0.000000 0\n",
      "7.660101 17\n",
      "0.000000 0\n",
      "8.418252 8\n",
      "7.959227 17\n",
      "8.097640 0\n",
      "7.778000 18\n",
      "7.661630 0\n",
      "7.991036 3\n",
      "8.497708 3\n",
      "7.933301 8\n",
      "7.623537 19\n",
      "7.864192 10\n",
      "7.799442 1\n",
      "7.948461 7\n",
      "7.806157 5\n",
      "7.637896 18\n",
      "7.745240 18\n"
     ]
    }
   ],
   "source": [
    "mdp6 = MDP('data/episodic-mdp-50-20.txt')\n",
    "V6 = mdp6.value_iteration()\n",
    "pi6 = mdp6.extract_policy(V6)\n",
    "for s in range(mdp6.num_states):\n",
    "    print(f\"{V6[s]:.6f} {pi6[s]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
